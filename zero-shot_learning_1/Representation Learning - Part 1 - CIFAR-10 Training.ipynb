{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Learning - Part 1 - CIAFAR-10 Training\n",
    "\n",
    "This notebook will detail the steps taken to learn representations of CIFAR-10 classes with two distinct data sets.\n",
    "\n",
    "Eventhough there is one class contained in CIFAR-1o that is not in the QuickDraw set (the 'deer' class), we will use all 10 classes. THis will be done in the hopes that more training data may make the network trained on CIFAR-10 learn useful features that help to separate the 9 classes that do have an equivalent in QuickDraw.\n",
    "\n",
    "We will now borrow heavily from the keras load_data() method from the cifar10 datasets module and the get_file() method from the data_utils module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 - Keras CNN with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as in the cifar10 CNN example from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858.0\n",
      "Trainable params: 1,250,858.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define out image processing flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# we will do some mild image pre-processing for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some callbacks, compile and get the training started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1562/1562 [==============================] - 156s - loss: 1.8752 - acc: 0.3084 - val_loss: 1.5989 - val_acc: 0.4170\n",
      "Epoch 2/200\n",
      "1562/1562 [==============================] - 148s - loss: 1.6115 - acc: 0.4084 - val_loss: 1.3964 - val_acc: 0.4985\n",
      "Epoch 3/200\n",
      "1562/1562 [==============================] - 145s - loss: 1.4819 - acc: 0.4605 - val_loss: 1.3021 - val_acc: 0.5360\n",
      "Epoch 4/200\n",
      "1562/1562 [==============================] - 144s - loss: 1.3979 - acc: 0.4962 - val_loss: 1.2167 - val_acc: 0.5698\n",
      "Epoch 5/200\n",
      "1562/1562 [==============================] - 144s - loss: 1.3299 - acc: 0.5205 - val_loss: 1.1594 - val_acc: 0.5901\n",
      "Epoch 6/200\n",
      "1562/1562 [==============================] - 145s - loss: 1.2721 - acc: 0.5456 - val_loss: 1.1138 - val_acc: 0.6039\n",
      "Epoch 7/200\n",
      "1562/1562 [==============================] - 141s - loss: 1.2261 - acc: 0.5650 - val_loss: 1.0454 - val_acc: 0.6308\n",
      "Epoch 8/200\n",
      "1562/1562 [==============================] - 141s - loss: 1.1894 - acc: 0.5769 - val_loss: 1.0154 - val_acc: 0.6402\n",
      "Epoch 9/200\n",
      "1562/1562 [==============================] - 141s - loss: 1.1589 - acc: 0.5870 - val_loss: 1.0021 - val_acc: 0.6487\n",
      "Epoch 10/200\n",
      "1562/1562 [==============================] - 141s - loss: 1.1327 - acc: 0.5988 - val_loss: 0.9690 - val_acc: 0.6566\n",
      "Epoch 11/200\n",
      "1562/1562 [==============================] - 141s - loss: 1.1066 - acc: 0.6093 - val_loss: 0.9674 - val_acc: 0.6635\n",
      "Epoch 12/200\n",
      "1562/1562 [==============================] - 141s - loss: 1.0842 - acc: 0.6185 - val_loss: 0.9040 - val_acc: 0.6777\n",
      "Epoch 13/200\n",
      "1562/1562 [==============================] - 141s - loss: 1.0671 - acc: 0.6249 - val_loss: 0.8824 - val_acc: 0.6871\n",
      "Epoch 14/200\n",
      "1562/1562 [==============================] - 141s - loss: 1.0437 - acc: 0.6329 - val_loss: 0.8757 - val_acc: 0.6942\n",
      "Epoch 15/200\n",
      "1562/1562 [==============================] - 142s - loss: 1.0303 - acc: 0.6385 - val_loss: 0.8737 - val_acc: 0.6935\n",
      "Epoch 16/200\n",
      "1562/1562 [==============================] - 141s - loss: 1.0121 - acc: 0.6428 - val_loss: 0.8763 - val_acc: 0.6931\n",
      "Epoch 17/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.9959 - acc: 0.6496 - val_loss: 0.8611 - val_acc: 0.6983\n",
      "Epoch 18/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.9879 - acc: 0.6547 - val_loss: 0.8127 - val_acc: 0.7179\n",
      "Epoch 19/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.9715 - acc: 0.6595 - val_loss: 0.8154 - val_acc: 0.7165\n",
      "Epoch 20/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.9639 - acc: 0.6623 - val_loss: 0.7969 - val_acc: 0.7225\n",
      "Epoch 21/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.9503 - acc: 0.6680 - val_loss: 0.8164 - val_acc: 0.7201\n",
      "Epoch 22/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.9468 - acc: 0.6706 - val_loss: 0.8000 - val_acc: 0.7286\n",
      "Epoch 23/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.9366 - acc: 0.6757 - val_loss: 0.7954 - val_acc: 0.7278\n",
      "Epoch 24/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.9289 - acc: 0.6773 - val_loss: 0.7564 - val_acc: 0.7411\n",
      "Epoch 25/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.9254 - acc: 0.6782 - val_loss: 0.7530 - val_acc: 0.7391\n",
      "Epoch 26/200\n",
      "1562/1562 [==============================] - 140s - loss: 0.9127 - acc: 0.6820 - val_loss: 0.7553 - val_acc: 0.7403\n",
      "Epoch 27/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.9134 - acc: 0.6804 - val_loss: 0.7384 - val_acc: 0.7484\n",
      "Epoch 28/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.9048 - acc: 0.6860 - val_loss: 0.7485 - val_acc: 0.7451\n",
      "Epoch 29/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8973 - acc: 0.6880 - val_loss: 0.7576 - val_acc: 0.7402\n",
      "Epoch 30/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.9012 - acc: 0.6882 - val_loss: 0.7548 - val_acc: 0.7386\n",
      "Epoch 31/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8874 - acc: 0.6924 - val_loss: 0.7303 - val_acc: 0.7495\n",
      "Epoch 32/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8846 - acc: 0.6932 - val_loss: 0.7270 - val_acc: 0.7558\n",
      "Epoch 33/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8836 - acc: 0.6951 - val_loss: 0.7076 - val_acc: 0.7601\n",
      "Epoch 34/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8769 - acc: 0.6977 - val_loss: 0.7186 - val_acc: 0.7564\n",
      "Epoch 35/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8765 - acc: 0.6966 - val_loss: 0.6975 - val_acc: 0.7598\n",
      "Epoch 36/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8725 - acc: 0.7026 - val_loss: 0.7071 - val_acc: 0.7599\n",
      "Epoch 37/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8709 - acc: 0.7021 - val_loss: 0.7209 - val_acc: 0.7560\n",
      "Epoch 38/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8661 - acc: 0.7056 - val_loss: 0.6780 - val_acc: 0.7644\n",
      "Epoch 39/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8686 - acc: 0.7015 - val_loss: 0.7030 - val_acc: 0.7672\n",
      "Epoch 40/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8628 - acc: 0.7063 - val_loss: 0.7007 - val_acc: 0.7617\n",
      "Epoch 41/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8618 - acc: 0.7060 - val_loss: 0.7050 - val_acc: 0.7549\n",
      "Epoch 42/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8634 - acc: 0.7064 - val_loss: 0.6734 - val_acc: 0.7666\n",
      "Epoch 43/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8641 - acc: 0.7067 - val_loss: 0.7062 - val_acc: 0.7658\n",
      "Epoch 44/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8569 - acc: 0.7072 - val_loss: 0.6973 - val_acc: 0.7600\n",
      "Epoch 45/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8586 - acc: 0.7045 - val_loss: 0.7043 - val_acc: 0.7656\n",
      "Epoch 46/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8573 - acc: 0.7077 - val_loss: 0.6996 - val_acc: 0.7603\n",
      "Epoch 47/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8583 - acc: 0.7087 - val_loss: 0.6857 - val_acc: 0.7706\n",
      "Epoch 48/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8614 - acc: 0.7082 - val_loss: 0.6836 - val_acc: 0.7678\n",
      "Epoch 49/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8593 - acc: 0.7083 - val_loss: 0.6954 - val_acc: 0.7621\n",
      "Epoch 50/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8583 - acc: 0.7079 - val_loss: 0.6991 - val_acc: 0.7618\n",
      "Epoch 51/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8508 - acc: 0.7096 - val_loss: 0.6560 - val_acc: 0.7782\n",
      "Epoch 52/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8536 - acc: 0.7133 - val_loss: 0.6782 - val_acc: 0.7690\n",
      "Epoch 53/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8587 - acc: 0.7092 - val_loss: 0.6830 - val_acc: 0.7736\n",
      "Epoch 54/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8476 - acc: 0.7120 - val_loss: 0.6915 - val_acc: 0.7671\n",
      "Epoch 55/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8526 - acc: 0.7111 - val_loss: 0.6660 - val_acc: 0.7726\n",
      "Epoch 56/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8559 - acc: 0.7120 - val_loss: 0.6718 - val_acc: 0.7693\n",
      "Epoch 57/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8522 - acc: 0.7129 - val_loss: 0.7328 - val_acc: 0.7668\n",
      "Epoch 58/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8507 - acc: 0.7140 - val_loss: 0.7080 - val_acc: 0.7655\n",
      "Epoch 59/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8509 - acc: 0.7106 - val_loss: 0.7240 - val_acc: 0.7615\n",
      "Epoch 60/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8511 - acc: 0.7096 - val_loss: 0.6807 - val_acc: 0.7772\n",
      "Epoch 61/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8550 - acc: 0.7109 - val_loss: 0.7201 - val_acc: 0.7620\n",
      "Epoch 62/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8496 - acc: 0.7131 - val_loss: 0.6695 - val_acc: 0.7770\n",
      "Epoch 63/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.8494 - acc: 0.7154 - val_loss: 0.6688 - val_acc: 0.7778\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 136s - loss: 0.8511 - acc: 0.7116 - val_loss: 0.6840 - val_acc: 0.7758\n",
      "Epoch 65/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8517 - acc: 0.7144 - val_loss: 0.6902 - val_acc: 0.7792\n",
      "Epoch 66/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8538 - acc: 0.7115 - val_loss: 0.6658 - val_acc: 0.7736\n",
      "Epoch 67/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8522 - acc: 0.7157 - val_loss: 0.6725 - val_acc: 0.7756\n",
      "Epoch 68/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8481 - acc: 0.7146 - val_loss: 0.7097 - val_acc: 0.7667\n",
      "Epoch 69/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8535 - acc: 0.7117 - val_loss: 0.6665 - val_acc: 0.7833\n",
      "Epoch 70/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8538 - acc: 0.7127 - val_loss: 0.6770 - val_acc: 0.7728\n",
      "Epoch 71/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8484 - acc: 0.7152 - val_loss: 0.6737 - val_acc: 0.7827\n",
      "Epoch 72/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8614 - acc: 0.7124 - val_loss: 0.6770 - val_acc: 0.7736\n",
      "Epoch 73/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8521 - acc: 0.7153 - val_loss: 0.6918 - val_acc: 0.7652\n",
      "Epoch 74/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8591 - acc: 0.7102 - val_loss: 0.6853 - val_acc: 0.7772\n",
      "Epoch 75/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8601 - acc: 0.7119 - val_loss: 0.6934 - val_acc: 0.7684\n",
      "Epoch 76/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8520 - acc: 0.7140 - val_loss: 0.7236 - val_acc: 0.7634\n",
      "Epoch 77/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8535 - acc: 0.7164 - val_loss: 0.6843 - val_acc: 0.7651\n",
      "Epoch 78/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8602 - acc: 0.7100 - val_loss: 0.6691 - val_acc: 0.7857\n",
      "Epoch 79/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8582 - acc: 0.7121 - val_loss: 0.7329 - val_acc: 0.7677\n",
      "Epoch 80/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8628 - acc: 0.7114 - val_loss: 0.7421 - val_acc: 0.7518\n",
      "Epoch 81/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8552 - acc: 0.7122 - val_loss: 0.6770 - val_acc: 0.7787\n",
      "Epoch 82/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8584 - acc: 0.7131 - val_loss: 0.6829 - val_acc: 0.7695\n",
      "Epoch 83/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8631 - acc: 0.7126 - val_loss: 0.7367 - val_acc: 0.7582\n",
      "Epoch 84/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8618 - acc: 0.7145 - val_loss: 0.6848 - val_acc: 0.7804\n",
      "Epoch 85/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8631 - acc: 0.7107 - val_loss: 0.6770 - val_acc: 0.7797\n",
      "Epoch 86/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8598 - acc: 0.7123 - val_loss: 0.6767 - val_acc: 0.7774\n",
      "Epoch 87/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8691 - acc: 0.7109 - val_loss: 0.6797 - val_acc: 0.7684\n",
      "Epoch 88/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8700 - acc: 0.7104 - val_loss: 0.6872 - val_acc: 0.7805\n",
      "Epoch 89/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8726 - acc: 0.7111 - val_loss: 0.6342 - val_acc: 0.7875\n",
      "Epoch 90/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8694 - acc: 0.7115 - val_loss: 0.7063 - val_acc: 0.7732\n",
      "Epoch 91/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8747 - acc: 0.7101 - val_loss: 0.7093 - val_acc: 0.7583\n",
      "Epoch 92/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8704 - acc: 0.7128 - val_loss: 0.6626 - val_acc: 0.7866\n",
      "Epoch 93/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8785 - acc: 0.7072 - val_loss: 0.7455 - val_acc: 0.7667\n",
      "Epoch 94/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8700 - acc: 0.7104 - val_loss: 0.6707 - val_acc: 0.7756\n",
      "Epoch 95/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8817 - acc: 0.7069 - val_loss: 0.6791 - val_acc: 0.7793\n",
      "Epoch 96/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8759 - acc: 0.7098 - val_loss: 0.6779 - val_acc: 0.7799\n",
      "Epoch 97/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8863 - acc: 0.7067 - val_loss: 0.7184 - val_acc: 0.7810\n",
      "Epoch 98/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8752 - acc: 0.7089 - val_loss: 0.6724 - val_acc: 0.7850\n",
      "Epoch 99/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8802 - acc: 0.7085 - val_loss: 0.6808 - val_acc: 0.7823\n",
      "Epoch 100/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8840 - acc: 0.7086 - val_loss: 0.7175 - val_acc: 0.7595\n",
      "Epoch 101/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8925 - acc: 0.7053 - val_loss: 0.6835 - val_acc: 0.7791\n",
      "Epoch 102/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8850 - acc: 0.7056 - val_loss: 0.6964 - val_acc: 0.7746\n",
      "Epoch 103/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8931 - acc: 0.7058 - val_loss: 0.7590 - val_acc: 0.7622\n",
      "Epoch 104/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8958 - acc: 0.7025 - val_loss: 0.6901 - val_acc: 0.7734\n",
      "Epoch 105/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8942 - acc: 0.7037 - val_loss: 0.7183 - val_acc: 0.7769\n",
      "Epoch 106/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.8920 - acc: 0.7034 - val_loss: 0.6933 - val_acc: 0.7754\n",
      "Epoch 107/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9050 - acc: 0.6996 - val_loss: 0.7446 - val_acc: 0.7670\n",
      "Epoch 108/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9026 - acc: 0.7007 - val_loss: 0.7704 - val_acc: 0.7546\n",
      "Epoch 109/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9104 - acc: 0.7006 - val_loss: 0.7185 - val_acc: 0.7751\n",
      "Epoch 110/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9079 - acc: 0.7005 - val_loss: 0.7434 - val_acc: 0.7606\n",
      "Epoch 111/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9112 - acc: 0.7025 - val_loss: 0.8292 - val_acc: 0.7433\n",
      "Epoch 112/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9136 - acc: 0.6984 - val_loss: 0.7500 - val_acc: 0.7500\n",
      "Epoch 113/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9180 - acc: 0.6986 - val_loss: 0.7931 - val_acc: 0.7369\n",
      "Epoch 114/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9229 - acc: 0.6941 - val_loss: 0.8200 - val_acc: 0.7630\n",
      "Epoch 115/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9307 - acc: 0.6950 - val_loss: 0.7037 - val_acc: 0.7700\n",
      "Epoch 116/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9266 - acc: 0.6966 - val_loss: 0.7652 - val_acc: 0.7621\n",
      "Epoch 117/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9209 - acc: 0.6963 - val_loss: 0.7343 - val_acc: 0.7607\n",
      "Epoch 118/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9420 - acc: 0.6893 - val_loss: 0.7358 - val_acc: 0.7618\n",
      "Epoch 119/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9408 - acc: 0.6924 - val_loss: 0.7060 - val_acc: 0.7663\n",
      "Epoch 120/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9405 - acc: 0.6929 - val_loss: 0.7570 - val_acc: 0.7625\n",
      "Epoch 121/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9481 - acc: 0.6868 - val_loss: 0.7072 - val_acc: 0.7767\n",
      "Epoch 122/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9507 - acc: 0.6889 - val_loss: 0.7091 - val_acc: 0.7687\n",
      "Epoch 123/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9454 - acc: 0.6888 - val_loss: 0.8109 - val_acc: 0.7440\n",
      "Epoch 124/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9620 - acc: 0.6843 - val_loss: 0.7257 - val_acc: 0.7657\n",
      "Epoch 125/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9632 - acc: 0.6873 - val_loss: 0.7780 - val_acc: 0.7487\n",
      "Epoch 126/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.9669 - acc: 0.6822 - val_loss: 0.7376 - val_acc: 0.7647\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 137s - loss: 0.9772 - acc: 0.6822 - val_loss: 0.7842 - val_acc: 0.7493\n",
      "Epoch 128/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.9706 - acc: 0.6836 - val_loss: 0.7820 - val_acc: 0.7555\n",
      "Epoch 129/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.9875 - acc: 0.6763 - val_loss: 0.8586 - val_acc: 0.7340\n",
      "Epoch 130/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.9869 - acc: 0.6762 - val_loss: 0.8469 - val_acc: 0.7480\n",
      "Epoch 131/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.9955 - acc: 0.6755 - val_loss: 0.7354 - val_acc: 0.7542\n",
      "Epoch 132/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.9994 - acc: 0.6757 - val_loss: 0.7806 - val_acc: 0.7458\n",
      "Epoch 133/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.0086 - acc: 0.6716 - val_loss: 0.8014 - val_acc: 0.7295\n",
      "Epoch 134/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.0210 - acc: 0.6673 - val_loss: 0.8163 - val_acc: 0.7408\n",
      "Epoch 135/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.0236 - acc: 0.6661 - val_loss: 0.7746 - val_acc: 0.7508\n",
      "Epoch 136/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.0285 - acc: 0.6648 - val_loss: 0.8033 - val_acc: 0.7440\n",
      "Epoch 137/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.0333 - acc: 0.6634 - val_loss: 0.8236 - val_acc: 0.7315\n",
      "Epoch 138/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0418 - acc: 0.6654 - val_loss: 0.8594 - val_acc: 0.7306\n",
      "Epoch 139/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0568 - acc: 0.6547 - val_loss: 0.9918 - val_acc: 0.6619\n",
      "Epoch 140/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0445 - acc: 0.6613 - val_loss: 0.8184 - val_acc: 0.7485\n",
      "Epoch 141/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0543 - acc: 0.6579 - val_loss: 0.8342 - val_acc: 0.7488\n",
      "Epoch 142/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0674 - acc: 0.6534 - val_loss: 0.7824 - val_acc: 0.7468\n",
      "Epoch 143/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0789 - acc: 0.6498 - val_loss: 0.9098 - val_acc: 0.7227\n",
      "Epoch 144/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.0843 - acc: 0.6503 - val_loss: 0.9223 - val_acc: 0.7029\n",
      "Epoch 145/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0860 - acc: 0.6462 - val_loss: 0.8582 - val_acc: 0.7317\n",
      "Epoch 146/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0932 - acc: 0.6449 - val_loss: 1.0022 - val_acc: 0.6967\n",
      "Epoch 147/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0977 - acc: 0.6441 - val_loss: 1.1438 - val_acc: 0.6290\n",
      "Epoch 148/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0980 - acc: 0.6459 - val_loss: 0.8780 - val_acc: 0.7246\n",
      "Epoch 149/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1104 - acc: 0.6417 - val_loss: 0.8562 - val_acc: 0.7286\n",
      "Epoch 150/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1172 - acc: 0.6394 - val_loss: 0.9166 - val_acc: 0.7270\n",
      "Epoch 151/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1283 - acc: 0.6341 - val_loss: 0.8999 - val_acc: 0.7211\n",
      "Epoch 152/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1399 - acc: 0.6315 - val_loss: 0.9694 - val_acc: 0.6923\n",
      "Epoch 153/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1484 - acc: 0.6287 - val_loss: 0.8186 - val_acc: 0.7324\n",
      "Epoch 154/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1583 - acc: 0.6244 - val_loss: 0.8969 - val_acc: 0.7289\n",
      "Epoch 155/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1778 - acc: 0.6183 - val_loss: 1.0264 - val_acc: 0.7094\n",
      "Epoch 156/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1743 - acc: 0.6202 - val_loss: 1.0856 - val_acc: 0.6499\n",
      "Epoch 157/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1844 - acc: 0.6178 - val_loss: 0.9999 - val_acc: 0.6901\n",
      "Epoch 158/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2017 - acc: 0.6102 - val_loss: 1.1269 - val_acc: 0.6969\n",
      "Epoch 159/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1991 - acc: 0.6145 - val_loss: 1.0110 - val_acc: 0.6975\n",
      "Epoch 160/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2069 - acc: 0.6116 - val_loss: 1.1277 - val_acc: 0.6321\n",
      "Epoch 161/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2076 - acc: 0.6095 - val_loss: 1.1351 - val_acc: 0.6663\n",
      "Epoch 162/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2269 - acc: 0.6039 - val_loss: 1.0147 - val_acc: 0.6878\n",
      "Epoch 163/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2370 - acc: 0.6004 - val_loss: 0.8547 - val_acc: 0.7274\n",
      "Epoch 164/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2284 - acc: 0.6041 - val_loss: 0.8908 - val_acc: 0.7137\n",
      "Epoch 165/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2344 - acc: 0.6008 - val_loss: 1.0390 - val_acc: 0.6553\n",
      "Epoch 166/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2632 - acc: 0.5912 - val_loss: 1.1041 - val_acc: 0.6768\n",
      "Epoch 167/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2434 - acc: 0.5972 - val_loss: 1.1591 - val_acc: 0.6393\n",
      "Epoch 168/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2602 - acc: 0.5921 - val_loss: 1.0568 - val_acc: 0.6689\n",
      "Epoch 169/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2694 - acc: 0.5884 - val_loss: 1.0645 - val_acc: 0.6626\n",
      "Epoch 170/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2745 - acc: 0.5887 - val_loss: 0.9751 - val_acc: 0.6660\n",
      "Epoch 171/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2763 - acc: 0.5883 - val_loss: 1.0829 - val_acc: 0.6648\n",
      "Epoch 172/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2923 - acc: 0.5812 - val_loss: 1.2352 - val_acc: 0.6568\n",
      "Epoch 173/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2970 - acc: 0.5801 - val_loss: 1.1614 - val_acc: 0.6442\n",
      "Epoch 174/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3079 - acc: 0.5736 - val_loss: 1.0607 - val_acc: 0.6552\n",
      "Epoch 175/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3128 - acc: 0.5750 - val_loss: 1.0867 - val_acc: 0.6294\n",
      "Epoch 176/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3156 - acc: 0.5768 - val_loss: 1.1536 - val_acc: 0.6430\n",
      "Epoch 177/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3182 - acc: 0.5732 - val_loss: 1.1713 - val_acc: 0.6289\n",
      "Epoch 178/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3264 - acc: 0.5707 - val_loss: 1.3061 - val_acc: 0.5751\n",
      "Epoch 179/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3176 - acc: 0.5729 - val_loss: 1.0733 - val_acc: 0.6326\n",
      "Epoch 180/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3221 - acc: 0.5725 - val_loss: 1.4046 - val_acc: 0.5123\n",
      "Epoch 181/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3311 - acc: 0.5684 - val_loss: 1.1751 - val_acc: 0.6405\n",
      "Epoch 182/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3330 - acc: 0.5703 - val_loss: 1.2941 - val_acc: 0.5625\n",
      "Epoch 183/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3437 - acc: 0.5644 - val_loss: 1.3193 - val_acc: 0.6009\n",
      "Epoch 184/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3498 - acc: 0.5628 - val_loss: 1.3077 - val_acc: 0.6450\n",
      "Epoch 185/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3527 - acc: 0.5626 - val_loss: 1.1446 - val_acc: 0.6328\n",
      "Epoch 186/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3475 - acc: 0.5627 - val_loss: 1.0007 - val_acc: 0.6752\n",
      "Epoch 187/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3555 - acc: 0.5619 - val_loss: 1.2111 - val_acc: 0.5887\n",
      "Epoch 188/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3498 - acc: 0.5622 - val_loss: 1.3709 - val_acc: 0.5948\n",
      "Epoch 189/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3416 - acc: 0.5666 - val_loss: 1.0830 - val_acc: 0.6499\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 137s - loss: 1.3503 - acc: 0.5613 - val_loss: 1.3844 - val_acc: 0.5768\n",
      "Epoch 191/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.3531 - acc: 0.5624 - val_loss: 1.2280 - val_acc: 0.6183\n",
      "Epoch 192/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.3563 - acc: 0.5601 - val_loss: 1.2126 - val_acc: 0.6014\n",
      "Epoch 193/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.3670 - acc: 0.5598 - val_loss: 1.1747 - val_acc: 0.6443\n",
      "Epoch 194/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.3625 - acc: 0.5544 - val_loss: 1.3267 - val_acc: 0.5606\n",
      "Epoch 195/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.3804 - acc: 0.5532 - val_loss: 1.3706 - val_acc: 0.5485\n",
      "Epoch 196/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.3722 - acc: 0.5542 - val_loss: 1.3738 - val_acc: 0.5498\n",
      "Epoch 197/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.3673 - acc: 0.5559 - val_loss: 1.1425 - val_acc: 0.6477\n",
      "Epoch 198/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.3698 - acc: 0.5558 - val_loss: 1.2185 - val_acc: 0.5885\n",
      "Epoch 199/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.3776 - acc: 0.5515 - val_loss: 1.3203 - val_acc: 0.5761\n",
      "Epoch 200/200\n",
      "1562/1562 [==============================] - 137s - loss: 1.3802 - acc: 0.5519 - val_loss: 1.4149 - val_acc: 0.5182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcee2ef4748>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "filepath = 'v1-weights.{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "model_chk = ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
    "                save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "              patience=5, min_lr=0.001)\n",
    "\n",
    "csv_log = CSVLogger('v1-training.log')\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[reduce_lr, model_chk, csv_log])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3 - FoDL CNN with Batch Normalization and Dropout\n",
    "\n",
    "Network design taken from Fundamentals of Deep Learning by Nikhil Buduma (O'Reilly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as in the cifar10 CNN example from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are using BatchNormalization in TensorFlow, we set this the axis to 3\n",
    "bn_axis = 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), padding='same', input_shape=x_train.shape[1:], name='conv1'))\n",
    "model.add(BatchNormalization(axis=bn_axis, name='bn_conv1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), padding='same', name='conv2'))\n",
    "model.add(BatchNormalization(axis=bn_axis, name='bn_conv2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(384, name='fc1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(192, name='fc2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define out image processing flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# we will do some mild image pre-processing for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some callbacks, compile and get the training started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 331s - loss: 1.7229 - acc: 0.3748 - val_loss: 1.5415 - val_acc: 0.4311\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 311s - loss: 1.3985 - acc: 0.4947 - val_loss: 1.3988 - val_acc: 0.5075\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 317s - loss: 1.2788 - acc: 0.5455 - val_loss: 1.6261 - val_acc: 0.4446\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 327s - loss: 1.1849 - acc: 0.5836 - val_loss: 1.1321 - val_acc: 0.6154\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 322s - loss: 1.1265 - acc: 0.6070 - val_loss: 1.0465 - val_acc: 0.6258\n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 315s - loss: 1.0789 - acc: 0.6217 - val_loss: 0.9948 - val_acc: 0.6486\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 327s - loss: 1.0339 - acc: 0.6417 - val_loss: 1.0292 - val_acc: 0.6371\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 327s - loss: 1.0050 - acc: 0.6517 - val_loss: 1.0462 - val_acc: 0.6465\n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.9734 - acc: 0.6633 - val_loss: 1.6285 - val_acc: 0.5276\n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.9539 - acc: 0.6715 - val_loss: 0.8353 - val_acc: 0.7222\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.9345 - acc: 0.6775 - val_loss: 1.1119 - val_acc: 0.6060\n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.9188 - acc: 0.6852 - val_loss: 0.8214 - val_acc: 0.7195\n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.9025 - acc: 0.6908 - val_loss: 0.8473 - val_acc: 0.7096\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.8856 - acc: 0.6963 - val_loss: 1.1400 - val_acc: 0.6372\n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.8781 - acc: 0.6980 - val_loss: 0.8687 - val_acc: 0.6980\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.8611 - acc: 0.7048 - val_loss: 0.8481 - val_acc: 0.7190\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.8530 - acc: 0.7073 - val_loss: 0.7458 - val_acc: 0.7457\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.8427 - acc: 0.7134 - val_loss: 0.8106 - val_acc: 0.7249\n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.8317 - acc: 0.7150 - val_loss: 0.8175 - val_acc: 0.7186\n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 301s - loss: 0.8179 - acc: 0.7189 - val_loss: 0.7512 - val_acc: 0.7402\n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.8049 - acc: 0.7253 - val_loss: 0.6706 - val_acc: 0.7754\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7979 - acc: 0.7284 - val_loss: 0.8138 - val_acc: 0.7207\n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7899 - acc: 0.7304 - val_loss: 0.8028 - val_acc: 0.7149\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7830 - acc: 0.7318 - val_loss: 0.7972 - val_acc: 0.7282\n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7741 - acc: 0.7353 - val_loss: 0.7031 - val_acc: 0.7516\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7618 - acc: 0.7397 - val_loss: 0.6312 - val_acc: 0.7856\n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7678 - acc: 0.7372 - val_loss: 0.6993 - val_acc: 0.7631\n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7578 - acc: 0.7449 - val_loss: 0.9019 - val_acc: 0.7063\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7458 - acc: 0.7446 - val_loss: 0.7746 - val_acc: 0.7404\n",
      "Epoch 30/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7391 - acc: 0.7477 - val_loss: 0.7676 - val_acc: 0.7362\n",
      "Epoch 31/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7370 - acc: 0.7475 - val_loss: 0.6848 - val_acc: 0.7667\n",
      "Epoch 32/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7254 - acc: 0.7508 - val_loss: 0.6048 - val_acc: 0.7931\n",
      "Epoch 33/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7197 - acc: 0.7549 - val_loss: 0.7373 - val_acc: 0.7422\n",
      "Epoch 34/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7191 - acc: 0.7569 - val_loss: 0.7048 - val_acc: 0.7594\n",
      "Epoch 35/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7156 - acc: 0.7565 - val_loss: 0.6095 - val_acc: 0.7946\n",
      "Epoch 36/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7058 - acc: 0.7598 - val_loss: 0.6755 - val_acc: 0.7711\n",
      "Epoch 37/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.7066 - acc: 0.7590 - val_loss: 0.5842 - val_acc: 0.8052\n",
      "Epoch 38/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6963 - acc: 0.7626 - val_loss: 0.7198 - val_acc: 0.7549\n",
      "Epoch 39/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6921 - acc: 0.7636 - val_loss: 0.6043 - val_acc: 0.7975\n",
      "Epoch 40/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6896 - acc: 0.7634 - val_loss: 0.5821 - val_acc: 0.8007\n",
      "Epoch 41/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6874 - acc: 0.7675 - val_loss: 0.7793 - val_acc: 0.7405\n",
      "Epoch 42/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6747 - acc: 0.7679 - val_loss: 0.6219 - val_acc: 0.7870\n",
      "Epoch 43/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6791 - acc: 0.7702 - val_loss: 0.6325 - val_acc: 0.7888\n",
      "Epoch 44/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6753 - acc: 0.7695 - val_loss: 0.5961 - val_acc: 0.7949\n",
      "Epoch 45/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6702 - acc: 0.7712 - val_loss: 0.6031 - val_acc: 0.7907\n",
      "Epoch 46/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6729 - acc: 0.7705 - val_loss: 0.6745 - val_acc: 0.7716\n",
      "Epoch 47/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6591 - acc: 0.7756 - val_loss: 0.6269 - val_acc: 0.7817\n",
      "Epoch 48/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6578 - acc: 0.7777 - val_loss: 0.7706 - val_acc: 0.7365\n",
      "Epoch 49/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6552 - acc: 0.7782 - val_loss: 0.5715 - val_acc: 0.8054\n",
      "Epoch 50/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.6521 - acc: 0.7750 - val_loss: 0.6342 - val_acc: 0.7831\n",
      "Epoch 51/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6507 - acc: 0.7784 - val_loss: 0.5937 - val_acc: 0.7998\n",
      "Epoch 52/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6468 - acc: 0.7794 - val_loss: 0.5561 - val_acc: 0.8132\n",
      "Epoch 53/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6419 - acc: 0.7829 - val_loss: 0.6787 - val_acc: 0.7692\n",
      "Epoch 54/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6307 - acc: 0.7840 - val_loss: 0.6101 - val_acc: 0.7884\n",
      "Epoch 55/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6359 - acc: 0.7809 - val_loss: 0.5655 - val_acc: 0.8068\n",
      "Epoch 56/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6311 - acc: 0.7847 - val_loss: 0.7486 - val_acc: 0.7533\n",
      "Epoch 57/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6311 - acc: 0.7820 - val_loss: 0.5303 - val_acc: 0.8218\n",
      "Epoch 58/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6301 - acc: 0.7854 - val_loss: 0.6111 - val_acc: 0.7939\n",
      "Epoch 59/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6263 - acc: 0.7864 - val_loss: 0.5695 - val_acc: 0.8116\n",
      "Epoch 60/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6208 - acc: 0.7879 - val_loss: 0.5947 - val_acc: 0.7967\n",
      "Epoch 61/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6223 - acc: 0.7883 - val_loss: 0.5314 - val_acc: 0.8179\n",
      "Epoch 62/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6243 - acc: 0.7888 - val_loss: 0.5263 - val_acc: 0.8224\n",
      "Epoch 63/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.6165 - acc: 0.7914 - val_loss: 0.5417 - val_acc: 0.8202\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 296s - loss: 0.6126 - acc: 0.7915 - val_loss: 0.5458 - val_acc: 0.8139\n",
      "Epoch 65/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.6155 - acc: 0.7897 - val_loss: 0.5860 - val_acc: 0.8005\n",
      "Epoch 66/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.6102 - acc: 0.7916 - val_loss: 0.5989 - val_acc: 0.7989\n",
      "Epoch 67/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.6060 - acc: 0.7925 - val_loss: 0.5283 - val_acc: 0.8227\n",
      "Epoch 68/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.6093 - acc: 0.7928 - val_loss: 0.5747 - val_acc: 0.8072\n",
      "Epoch 69/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.6062 - acc: 0.7919 - val_loss: 0.5581 - val_acc: 0.8121\n",
      "Epoch 70/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5980 - acc: 0.7967 - val_loss: 0.4917 - val_acc: 0.8328\n",
      "Epoch 71/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.6028 - acc: 0.7959 - val_loss: 0.5657 - val_acc: 0.8079\n",
      "Epoch 72/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.5927 - acc: 0.7973 - val_loss: 0.5120 - val_acc: 0.8245\n",
      "Epoch 73/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.5916 - acc: 0.7985 - val_loss: 0.5230 - val_acc: 0.8234\n",
      "Epoch 74/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.5884 - acc: 0.7987 - val_loss: 0.5378 - val_acc: 0.8176\n",
      "Epoch 75/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.5916 - acc: 0.7985 - val_loss: 0.5170 - val_acc: 0.8202\n",
      "Epoch 76/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.5879 - acc: 0.8005 - val_loss: 0.5089 - val_acc: 0.8278\n",
      "Epoch 77/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.5883 - acc: 0.8000 - val_loss: 0.5524 - val_acc: 0.8124\n",
      "Epoch 78/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.5837 - acc: 0.8032 - val_loss: 0.5287 - val_acc: 0.8205\n",
      "Epoch 79/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.5825 - acc: 0.8003 - val_loss: 0.5498 - val_acc: 0.8159\n",
      "Epoch 80/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.5821 - acc: 0.8028 - val_loss: 0.5279 - val_acc: 0.8211\n",
      "Epoch 81/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5833 - acc: 0.8014 - val_loss: 0.5510 - val_acc: 0.8130\n",
      "Epoch 82/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5821 - acc: 0.8031 - val_loss: 0.5588 - val_acc: 0.8109\n",
      "Epoch 83/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5803 - acc: 0.8013 - val_loss: 0.5152 - val_acc: 0.8260\n",
      "Epoch 84/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5819 - acc: 0.8022 - val_loss: 0.4997 - val_acc: 0.8304\n",
      "Epoch 85/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5713 - acc: 0.8051 - val_loss: 0.5072 - val_acc: 0.8276\n",
      "Epoch 86/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5690 - acc: 0.8046 - val_loss: 0.5466 - val_acc: 0.8144\n",
      "Epoch 87/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5702 - acc: 0.8059 - val_loss: 0.5279 - val_acc: 0.8230\n",
      "Epoch 88/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5675 - acc: 0.8054 - val_loss: 0.5633 - val_acc: 0.8111\n",
      "Epoch 89/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5622 - acc: 0.8103 - val_loss: 0.5882 - val_acc: 0.8081\n",
      "Epoch 90/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5583 - acc: 0.8086 - val_loss: 0.5229 - val_acc: 0.8234\n",
      "Epoch 91/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5679 - acc: 0.8071 - val_loss: 0.4949 - val_acc: 0.8347\n",
      "Epoch 92/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5609 - acc: 0.8092 - val_loss: 0.6439 - val_acc: 0.7884\n",
      "Epoch 93/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5614 - acc: 0.8075 - val_loss: 0.5335 - val_acc: 0.8211\n",
      "Epoch 94/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5637 - acc: 0.8078 - val_loss: 0.5300 - val_acc: 0.8216\n",
      "Epoch 95/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5549 - acc: 0.8088 - val_loss: 0.5183 - val_acc: 0.8244\n",
      "Epoch 96/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5545 - acc: 0.8124 - val_loss: 0.6196 - val_acc: 0.7968\n",
      "Epoch 97/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5552 - acc: 0.8097 - val_loss: 0.5262 - val_acc: 0.8231\n",
      "Epoch 98/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5571 - acc: 0.8110 - val_loss: 0.4971 - val_acc: 0.8317\n",
      "Epoch 99/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5484 - acc: 0.8133 - val_loss: 0.4783 - val_acc: 0.8363\n",
      "Epoch 100/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.5513 - acc: 0.8115 - val_loss: 0.5389 - val_acc: 0.8150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60f7d6ec88>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# initiate Adam opt1mizer\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "#opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "filepath = 'v3-weights.{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "model_chk = ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
    "                save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "csv_log = CSVLogger('v3-training.log')\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[model_chk, csv_log])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V4 - FoDL CNN with Batch Normalization Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as in the cifar10 CNN example from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since we are using BatchNormalization in TensorFlow, we set this the axis to 3\n",
    "bn_axis = 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), padding='same', input_shape=x_train.shape[1:], name='conv1'))\n",
    "model.add(BatchNormalization(axis=3, name='bn_conv1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), padding='same', name='conv2'))\n",
    "model.add(BatchNormalization(axis=3, name='bn_conv2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(384, name='fc1'))\n",
    "model.add(BatchNormalization(axis=1, name='bn_fc1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(192, name='fc2'))\n",
    "model.add(BatchNormalization(axis=1, name='bn_fc2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes, name='output'))\n",
    "model.add(BatchNormalization(axis=1, name='bn_outptu'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define out image processing flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# we will do some mild image pre-processing for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some callbacks, compile and get the training started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 332s - loss: 1.4255 - acc: 0.5007 - val_loss: 1.5459 - val_acc: 0.4638\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 331s - loss: 1.1445 - acc: 0.6004 - val_loss: 1.3762 - val_acc: 0.5227: 5s - \n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 329s - loss: 1.0280 - acc: 0.6424 - val_loss: 1.4590 - val_acc: 0.4983\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 308s - loss: 0.9427 - acc: 0.6714 - val_loss: 0.8671 - val_acc: 0.7002\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.8862 - acc: 0.6917 - val_loss: 0.9696 - val_acc: 0.6601\n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 317s - loss: 0.8355 - acc: 0.7114 - val_loss: 0.7672 - val_acc: 0.7377\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 326s - loss: 0.7961 - acc: 0.7243 - val_loss: 0.7060 - val_acc: 0.7580\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 328s - loss: 0.7677 - acc: 0.7333 - val_loss: 0.7392 - val_acc: 0.7428\n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 330s - loss: 0.7321 - acc: 0.7480 - val_loss: 0.7927 - val_acc: 0.7257\n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 326s - loss: 0.7116 - acc: 0.7547 - val_loss: 0.7040 - val_acc: 0.7594\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 326s - loss: 0.6883 - acc: 0.7606 - val_loss: 0.7235 - val_acc: 0.7533\n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 329s - loss: 0.6661 - acc: 0.7692 - val_loss: 0.7014 - val_acc: 0.7643768 - ETA: 6s - loss: 0.6668 - acc: 0.76 - ETA: 6s - loss: 0.666 - ETA: 3s - loss: 0.66\n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 330s - loss: 0.6506 - acc: 0.7751 - val_loss: 0.6977 - val_acc: 0.7632\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.6304 - acc: 0.7813 - val_loss: 0.7103 - val_acc: 0.7594\n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.6176 - acc: 0.7862 - val_loss: 0.7917 - val_acc: 0.7445\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 327s - loss: 0.6047 - acc: 0.7899 - val_loss: 0.5554 - val_acc: 0.8073\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 328s - loss: 0.5848 - acc: 0.7996 - val_loss: 0.6524 - val_acc: 0.77417\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 328s - loss: 0.5703 - acc: 0.8025 - val_loss: 0.8268 - val_acc: 0.7252\n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 304s - loss: 0.5653 - acc: 0.8061 - val_loss: 0.6799 - val_acc: 0.7724\n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.5540 - acc: 0.8089 - val_loss: 0.7016 - val_acc: 0.7708\n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.5415 - acc: 0.8116 - val_loss: 0.7472 - val_acc: 0.7485\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.5293 - acc: 0.8161 - val_loss: 0.6086 - val_acc: 0.7949\n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.5192 - acc: 0.8202 - val_loss: 0.8618 - val_acc: 0.7166\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.5171 - acc: 0.8216 - val_loss: 0.5233 - val_acc: 0.8200\n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.5018 - acc: 0.8262 - val_loss: 0.6147 - val_acc: 0.7916\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.4977 - acc: 0.8279 - val_loss: 0.5393 - val_acc: 0.8186\n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.4846 - acc: 0.8329 - val_loss: 0.5769 - val_acc: 0.8100\n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.4744 - acc: 0.8353 - val_loss: 0.5694 - val_acc: 0.8100\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.4714 - acc: 0.8362 - val_loss: 0.5203 - val_acc: 0.8262\n",
      "Epoch 30/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.4649 - acc: 0.8409 - val_loss: 0.5747 - val_acc: 0.8054\n",
      "Epoch 31/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.4556 - acc: 0.8430 - val_loss: 0.5528 - val_acc: 0.8187\n",
      "Epoch 32/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.4530 - acc: 0.8447 - val_loss: 0.7075 - val_acc: 0.7737\n",
      "Epoch 33/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.4489 - acc: 0.8432 - val_loss: 0.5124 - val_acc: 0.8269\n",
      "Epoch 34/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.4353 - acc: 0.8505 - val_loss: 0.5548 - val_acc: 0.8199\n",
      "Epoch 35/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.4360 - acc: 0.8485 - val_loss: 0.5463 - val_acc: 0.8171\n",
      "Epoch 36/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.4268 - acc: 0.8526 - val_loss: 0.5212 - val_acc: 0.8270\n",
      "Epoch 37/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.4231 - acc: 0.8533 - val_loss: 0.6502 - val_acc: 0.7904\n",
      "Epoch 38/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.4149 - acc: 0.8569 - val_loss: 0.5933 - val_acc: 0.8035\n",
      "Epoch 39/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.4083 - acc: 0.8585 - val_loss: 0.5898 - val_acc: 0.8058\n",
      "Epoch 40/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.4089 - acc: 0.8594 - val_loss: 0.4975 - val_acc: 0.8349\n",
      "Epoch 41/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.4086 - acc: 0.8601 - val_loss: 0.5149 - val_acc: 0.8339\n",
      "Epoch 42/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.3969 - acc: 0.8626 - val_loss: 0.6015 - val_acc: 0.8035\n",
      "Epoch 43/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.3872 - acc: 0.8652 - val_loss: 0.5869 - val_acc: 0.8058\n",
      "Epoch 44/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.3859 - acc: 0.8669 - val_loss: 0.5206 - val_acc: 0.8284\n",
      "Epoch 45/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.3889 - acc: 0.8663 - val_loss: 0.5377 - val_acc: 0.8223\n",
      "Epoch 46/100\n",
      "1562/1562 [==============================] - 297s - loss: 0.3755 - acc: 0.8702 - val_loss: 0.5113 - val_acc: 0.8371\n",
      "Epoch 47/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.3753 - acc: 0.8708 - val_loss: 0.4842 - val_acc: 0.8393\n",
      "Epoch 48/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.3679 - acc: 0.8714 - val_loss: 0.5245 - val_acc: 0.8259\n",
      "Epoch 49/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.3648 - acc: 0.8727 - val_loss: 0.5533 - val_acc: 0.8178\n",
      "Epoch 50/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.3614 - acc: 0.8748 - val_loss: 0.6317 - val_acc: 0.7985\n",
      "Epoch 51/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.3569 - acc: 0.8773 - val_loss: 0.6065 - val_acc: 0.8114\n",
      "Epoch 52/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.3507 - acc: 0.8792 - val_loss: 0.5767 - val_acc: 0.8151\n",
      "Epoch 53/100\n",
      "1562/1562 [==============================] - 298s - loss: 0.3544 - acc: 0.8767 - val_loss: 0.5564 - val_acc: 0.8264\n",
      "Epoch 54/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.3437 - acc: 0.8811 - val_loss: 0.5055 - val_acc: 0.8347\n",
      "Epoch 55/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.3496 - acc: 0.8786 - val_loss: 0.5982 - val_acc: 0.8122\n",
      "Epoch 56/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.3398 - acc: 0.8834 - val_loss: 0.5755 - val_acc: 0.8169\n",
      "Epoch 57/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.3412 - acc: 0.8817 - val_loss: 0.4875 - val_acc: 0.8427\n",
      "Epoch 58/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.3345 - acc: 0.8845 - val_loss: 0.5197 - val_acc: 0.8356\n",
      "Epoch 59/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.3370 - acc: 0.8831 - val_loss: 0.5356 - val_acc: 0.8290\n",
      "Epoch 60/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.3257 - acc: 0.8873 - val_loss: 0.5759 - val_acc: 0.8188\n",
      "Epoch 61/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.3288 - acc: 0.8866 - val_loss: 0.5310 - val_acc: 0.8345\n",
      "Epoch 62/100\n",
      "1562/1562 [==============================] - 299s - loss: 0.3182 - acc: 0.8905 - val_loss: 0.4895 - val_acc: 0.8450\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 299s - loss: 0.3219 - acc: 0.8881 - val_loss: 0.5520 - val_acc: 0.8246\n",
      "Epoch 64/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.3151 - acc: 0.8900 - val_loss: 0.5272 - val_acc: 0.8373\n",
      "Epoch 65/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.3171 - acc: 0.8907 - val_loss: 0.5137 - val_acc: 0.8430\n",
      "Epoch 66/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.3127 - acc: 0.8929 - val_loss: 0.5082 - val_acc: 0.8393\n",
      "Epoch 67/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.3112 - acc: 0.8926 - val_loss: 0.6278 - val_acc: 0.8162\n",
      "Epoch 68/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.3080 - acc: 0.8933 - val_loss: 0.5946 - val_acc: 0.8120\n",
      "Epoch 69/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2969 - acc: 0.8982 - val_loss: 0.5308 - val_acc: 0.8312\n",
      "Epoch 70/100\n",
      "1562/1562 [==============================] - 294s - loss: 0.2966 - acc: 0.8981 - val_loss: 0.5300 - val_acc: 0.8317\n",
      "Epoch 71/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.3005 - acc: 0.8956 - val_loss: 0.5183 - val_acc: 0.8399\n",
      "Epoch 72/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2930 - acc: 0.8988 - val_loss: 0.5488 - val_acc: 0.8288\n",
      "Epoch 73/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2925 - acc: 0.9000 - val_loss: 0.5024 - val_acc: 0.8406\n",
      "Epoch 74/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2887 - acc: 0.9026 - val_loss: 0.4852 - val_acc: 0.8490\n",
      "Epoch 75/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2884 - acc: 0.9011 - val_loss: 0.5109 - val_acc: 0.8415\n",
      "Epoch 76/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2880 - acc: 0.9016 - val_loss: 0.5285 - val_acc: 0.8346\n",
      "Epoch 77/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2878 - acc: 0.9001 - val_loss: 0.5261 - val_acc: 0.8370\n",
      "Epoch 78/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2826 - acc: 0.9036 - val_loss: 0.5618 - val_acc: 0.8336\n",
      "Epoch 79/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2823 - acc: 0.9024 - val_loss: 0.5448 - val_acc: 0.8333\n",
      "Epoch 80/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2777 - acc: 0.9046 - val_loss: 0.5816 - val_acc: 0.8217\n",
      "Epoch 81/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2724 - acc: 0.9048 - val_loss: 0.5175 - val_acc: 0.8424\n",
      "Epoch 82/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2753 - acc: 0.9051 - val_loss: 0.4942 - val_acc: 0.8440\n",
      "Epoch 83/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2708 - acc: 0.9061 - val_loss: 0.4844 - val_acc: 0.8544\n",
      "Epoch 84/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2692 - acc: 0.9079 - val_loss: 0.5431 - val_acc: 0.8332\n",
      "Epoch 85/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2751 - acc: 0.9045 - val_loss: 0.5141 - val_acc: 0.8384\n",
      "Epoch 86/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2664 - acc: 0.9076 - val_loss: 0.5453 - val_acc: 0.8365\n",
      "Epoch 87/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.2596 - acc: 0.9097 - val_loss: 0.5719 - val_acc: 0.8281\n",
      "Epoch 88/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2618 - acc: 0.9107 - val_loss: 0.5158 - val_acc: 0.8443\n",
      "Epoch 89/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2581 - acc: 0.9105 - val_loss: 0.5263 - val_acc: 0.8423\n",
      "Epoch 90/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.2577 - acc: 0.9118 - val_loss: 0.5559 - val_acc: 0.8315\n",
      "Epoch 91/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.2597 - acc: 0.9112 - val_loss: 0.5919 - val_acc: 0.8261\n",
      "Epoch 92/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.2574 - acc: 0.9119 - val_loss: 0.5504 - val_acc: 0.8350\n",
      "Epoch 93/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2598 - acc: 0.9107 - val_loss: 0.5776 - val_acc: 0.8306\n",
      "Epoch 94/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2550 - acc: 0.9118 - val_loss: 0.5787 - val_acc: 0.8292\n",
      "Epoch 95/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2470 - acc: 0.9141 - val_loss: 0.5679 - val_acc: 0.8307\n",
      "Epoch 96/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.2442 - acc: 0.9153 - val_loss: 0.5708 - val_acc: 0.8308\n",
      "Epoch 97/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.2478 - acc: 0.9134 - val_loss: 0.5593 - val_acc: 0.8307\n",
      "Epoch 98/100\n",
      "1562/1562 [==============================] - 296s - loss: 0.2436 - acc: 0.9153 - val_loss: 0.5387 - val_acc: 0.8368\n",
      "Epoch 99/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2437 - acc: 0.9162 - val_loss: 0.5125 - val_acc: 0.8427\n",
      "Epoch 100/100\n",
      "1562/1562 [==============================] - 295s - loss: 0.2440 - acc: 0.9167 - val_loss: 0.5514 - val_acc: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd941932ac8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# initiate Adam opt1mizer\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "#opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "filepath = 'v4-weights.{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "model_chk = ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
    "                save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "csv_log = CSVLogger('v4-training.log')\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[model_chk, csv_log])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V5 - Keras CNN with Batch Normalization Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as in the cifar10 CNN example from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "bn_conv1 (BatchNormalization (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "bn_conv2 (BatchNormalization (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "bn_conv3 (BatchNormalization (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "bn_conv4 (BatchNormalization (None, 13, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "bn_fc1 (BatchNormalization)  (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "bn_outptut (BatchNormalizati (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,253,714.0\n",
      "Trainable params: 1,252,286.0\n",
      "Non-trainable params: 1,428.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# since we are using BatchNormalization in TensorFlow, we set this the axis to 3\n",
    "bn_axis = 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:], name='conv1'))\n",
    "model.add(BatchNormalization(axis=3, name='bn_conv1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), name='conv2'))\n",
    "model.add(BatchNormalization(axis=3, name='bn_conv2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', name='conv3'))\n",
    "model.add(BatchNormalization(axis=3, name='bn_conv3'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), name='conv4'))\n",
    "model.add(BatchNormalization(axis=3, name='bn_conv4'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, name='fc1'))\n",
    "model.add(BatchNormalization(axis=1, name='bn_fc1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes, name='output'))\n",
    "model.add(BatchNormalization(axis=1, name='bn_outptut'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define out image processing flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# we will do some mild image pre-processing for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some callbacks, compile and get the training started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 273s - loss: 1.3519 - acc: 0.5293 - val_loss: 1.3831 - val_acc: 0.5283\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 245s - loss: 1.0370 - acc: 0.6445 - val_loss: 1.2745 - val_acc: 0.5644\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 244s - loss: 0.9046 - acc: 0.6917 - val_loss: 0.9614 - val_acc: 0.6636\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.8341 - acc: 0.7132 - val_loss: 0.7704 - val_acc: 0.7340\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.7741 - acc: 0.7339 - val_loss: 1.1966 - val_acc: 0.6106\n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 245s - loss: 0.7326 - acc: 0.7472 - val_loss: 0.7136 - val_acc: 0.7553\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 246s - loss: 0.6984 - acc: 0.7583 - val_loss: 0.6204 - val_acc: 0.7886\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 247s - loss: 0.6697 - acc: 0.7692 - val_loss: 0.5909 - val_acc: 0.7968\n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 245s - loss: 0.6467 - acc: 0.7775 - val_loss: 0.6856 - val_acc: 0.7622\n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 248s - loss: 0.6210 - acc: 0.7882 - val_loss: 0.5420 - val_acc: 0.8171\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.5976 - acc: 0.7948 - val_loss: 0.4880 - val_acc: 0.8403\n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.5846 - acc: 0.7984 - val_loss: 0.4861 - val_acc: 0.8359\n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.5669 - acc: 0.8039 - val_loss: 0.5013 - val_acc: 0.8313\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.5587 - acc: 0.8081 - val_loss: 0.4701 - val_acc: 0.8403\n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.5363 - acc: 0.8151 - val_loss: 0.4937 - val_acc: 0.8316\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.5235 - acc: 0.8208 - val_loss: 0.4552 - val_acc: 0.8459\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.5071 - acc: 0.8270 - val_loss: 0.4620 - val_acc: 0.8456\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.5032 - acc: 0.8257 - val_loss: 0.4325 - val_acc: 0.8517\n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.4908 - acc: 0.8295 - val_loss: 0.5099 - val_acc: 0.8322\n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.4767 - acc: 0.8365 - val_loss: 0.4730 - val_acc: 0.8406\n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.4748 - acc: 0.8373 - val_loss: 0.4356 - val_acc: 0.8552\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.4676 - acc: 0.8390 - val_loss: 0.4413 - val_acc: 0.8561\n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.4564 - acc: 0.8429 - val_loss: 0.4386 - val_acc: 0.8515\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.4471 - acc: 0.8457 - val_loss: 0.4397 - val_acc: 0.8513\n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.4353 - acc: 0.8505 - val_loss: 0.4315 - val_acc: 0.8505\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.4334 - acc: 0.8509 - val_loss: 0.5262 - val_acc: 0.8211\n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.4238 - acc: 0.8542 - val_loss: 0.4353 - val_acc: 0.8507\n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.4200 - acc: 0.8559 - val_loss: 0.4812 - val_acc: 0.8412\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.4105 - acc: 0.8575 - val_loss: 0.4031 - val_acc: 0.8668\n",
      "Epoch 30/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.4065 - acc: 0.8595 - val_loss: 0.4468 - val_acc: 0.8495\n",
      "Epoch 31/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3971 - acc: 0.8617 - val_loss: 0.4217 - val_acc: 0.8619\n",
      "Epoch 32/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3946 - acc: 0.8632 - val_loss: 0.4020 - val_acc: 0.8654\n",
      "Epoch 33/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3832 - acc: 0.8683 - val_loss: 0.4120 - val_acc: 0.8602\n",
      "Epoch 34/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3820 - acc: 0.8704 - val_loss: 0.4340 - val_acc: 0.8548\n",
      "Epoch 35/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3768 - acc: 0.8700 - val_loss: 0.4289 - val_acc: 0.8567\n",
      "Epoch 36/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3694 - acc: 0.8717 - val_loss: 0.4318 - val_acc: 0.8603\n",
      "Epoch 37/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3712 - acc: 0.8721 - val_loss: 0.4418 - val_acc: 0.8545\n",
      "Epoch 38/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3537 - acc: 0.8770 - val_loss: 0.4664 - val_acc: 0.8459\n",
      "Epoch 39/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3618 - acc: 0.8738 - val_loss: 0.4134 - val_acc: 0.8630\n",
      "Epoch 40/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3519 - acc: 0.8782 - val_loss: 0.3866 - val_acc: 0.8737\n",
      "Epoch 41/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3488 - acc: 0.8788 - val_loss: 0.4037 - val_acc: 0.8648\n",
      "Epoch 42/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3434 - acc: 0.8839 - val_loss: 0.3942 - val_acc: 0.8696\n",
      "Epoch 43/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3358 - acc: 0.8847 - val_loss: 0.4194 - val_acc: 0.8608\n",
      "Epoch 44/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3360 - acc: 0.8847 - val_loss: 0.3933 - val_acc: 0.8672\n",
      "Epoch 45/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3317 - acc: 0.8847 - val_loss: 0.4017 - val_acc: 0.8673\n",
      "Epoch 46/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3255 - acc: 0.8877 - val_loss: 0.4054 - val_acc: 0.8709\n",
      "Epoch 47/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3267 - acc: 0.8865 - val_loss: 0.4326 - val_acc: 0.8572\n",
      "Epoch 48/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3221 - acc: 0.8880 - val_loss: 0.4285 - val_acc: 0.8594\n",
      "Epoch 49/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3193 - acc: 0.8906 - val_loss: 0.4077 - val_acc: 0.8687\n",
      "Epoch 50/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3148 - acc: 0.8928 - val_loss: 0.4474 - val_acc: 0.8558\n",
      "Epoch 51/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3072 - acc: 0.8937 - val_loss: 0.4405 - val_acc: 0.8591\n",
      "Epoch 52/100\n",
      "1562/1562 [==============================] - 241s - loss: 0.3068 - acc: 0.8935 - val_loss: 0.4598 - val_acc: 0.8527\n",
      "Epoch 53/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.3035 - acc: 0.8964 - val_loss: 0.3858 - val_acc: 0.8742\n",
      "Epoch 54/100\n",
      "1562/1562 [==============================] - 248s - loss: 0.3096 - acc: 0.8941 - val_loss: 0.4080 - val_acc: 0.8671\n",
      "Epoch 55/100\n",
      "1562/1562 [==============================] - 248s - loss: 0.2980 - acc: 0.8973 - val_loss: 0.4237 - val_acc: 0.8620\n",
      "Epoch 56/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2956 - acc: 0.8979 - val_loss: 0.3830 - val_acc: 0.8770\n",
      "Epoch 57/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2942 - acc: 0.9003 - val_loss: 0.4126 - val_acc: 0.8666\n",
      "Epoch 58/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2904 - acc: 0.9004 - val_loss: 0.4104 - val_acc: 0.8660\n",
      "Epoch 59/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2886 - acc: 0.8993 - val_loss: 0.3875 - val_acc: 0.8754\n",
      "Epoch 60/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2877 - acc: 0.9021 - val_loss: 0.4183 - val_acc: 0.8653\n",
      "Epoch 61/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2759 - acc: 0.9058 - val_loss: 0.4027 - val_acc: 0.8722\n",
      "Epoch 62/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2818 - acc: 0.9040 - val_loss: 0.4740 - val_acc: 0.8435\n",
      "Epoch 63/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2819 - acc: 0.9021 - val_loss: 0.3770 - val_acc: 0.8754\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 239s - loss: 0.2783 - acc: 0.9031 - val_loss: 0.3822 - val_acc: 0.8771\n",
      "Epoch 65/100\n",
      "1562/1562 [==============================] - 239s - loss: 0.2791 - acc: 0.9042 - val_loss: 0.4253 - val_acc: 0.8651\n",
      "Epoch 66/100\n",
      "1562/1562 [==============================] - 246s - loss: 0.2736 - acc: 0.9058 - val_loss: 0.4204 - val_acc: 0.8679\n",
      "Epoch 67/100\n",
      "1562/1562 [==============================] - 244s - loss: 0.2735 - acc: 0.9054 - val_loss: 0.4113 - val_acc: 0.8669\n",
      "Epoch 68/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.2675 - acc: 0.9067 - val_loss: 0.3861 - val_acc: 0.8770\n",
      "Epoch 69/100\n",
      "1562/1562 [==============================] - 240s - loss: 0.2655 - acc: 0.9083 - val_loss: 0.3790 - val_acc: 0.8805\n",
      "Epoch 70/100\n",
      "1562/1562 [==============================] - 251s - loss: 0.2641 - acc: 0.9081 - val_loss: 0.4288 - val_acc: 0.8645\n",
      "Epoch 71/100\n",
      "1562/1562 [==============================] - 256s - loss: 0.2604 - acc: 0.9103 - val_loss: 0.4214 - val_acc: 0.8641\n",
      "Epoch 72/100\n",
      "1562/1562 [==============================] - 247s - loss: 0.2639 - acc: 0.9097 - val_loss: 0.3969 - val_acc: 0.8716\n",
      "Epoch 73/100\n",
      "1562/1562 [==============================] - 251s - loss: 0.2557 - acc: 0.9110 - val_loss: 0.4295 - val_acc: 0.8620\n",
      "Epoch 74/100\n",
      "1562/1562 [==============================] - 246s - loss: 0.2540 - acc: 0.9113 - val_loss: 0.3830 - val_acc: 0.8801\n",
      "Epoch 75/100\n",
      "1562/1562 [==============================] - 245s - loss: 0.2526 - acc: 0.9118 - val_loss: 0.3856 - val_acc: 0.8768\n",
      "Epoch 76/100\n",
      "1562/1562 [==============================] - 247s - loss: 0.2505 - acc: 0.9143 - val_loss: 0.4143 - val_acc: 0.8672\n",
      "Epoch 77/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2515 - acc: 0.9127 - val_loss: 0.4161 - val_acc: 0.8704\n",
      "Epoch 78/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2505 - acc: 0.9133 - val_loss: 0.3823 - val_acc: 0.8797\n",
      "Epoch 79/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2407 - acc: 0.9172 - val_loss: 0.3920 - val_acc: 0.8775\n",
      "Epoch 80/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2485 - acc: 0.9124 - val_loss: 0.4157 - val_acc: 0.8710\n",
      "Epoch 81/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2454 - acc: 0.9144 - val_loss: 0.3887 - val_acc: 0.8770\n",
      "Epoch 82/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2432 - acc: 0.9163 - val_loss: 0.3749 - val_acc: 0.8831\n",
      "Epoch 83/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2375 - acc: 0.9178 - val_loss: 0.4179 - val_acc: 0.8718\n",
      "Epoch 84/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2454 - acc: 0.9161 - val_loss: 0.3924 - val_acc: 0.8757\n",
      "Epoch 85/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2396 - acc: 0.9173 - val_loss: 0.3757 - val_acc: 0.8807\n",
      "Epoch 86/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2349 - acc: 0.9188 - val_loss: 0.3791 - val_acc: 0.8811\n",
      "Epoch 87/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2345 - acc: 0.9184 - val_loss: 0.3825 - val_acc: 0.8789\n",
      "Epoch 88/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2343 - acc: 0.9191 - val_loss: 0.4024 - val_acc: 0.8766\n",
      "Epoch 89/100\n",
      "1562/1562 [==============================] - 244s - loss: 0.2318 - acc: 0.9201 - val_loss: 0.4726 - val_acc: 0.8516\n",
      "Epoch 90/100\n",
      "1562/1562 [==============================] - 246s - loss: 0.2340 - acc: 0.9193 - val_loss: 0.4121 - val_acc: 0.8738\n",
      "Epoch 91/100\n",
      "1562/1562 [==============================] - 246s - loss: 0.2282 - acc: 0.9196 - val_loss: 0.3865 - val_acc: 0.8801\n",
      "Epoch 92/100\n",
      "1562/1562 [==============================] - 246s - loss: 0.2302 - acc: 0.9213 - val_loss: 0.4004 - val_acc: 0.8747\n",
      "Epoch 93/100\n",
      "1562/1562 [==============================] - 246s - loss: 0.2258 - acc: 0.9220 - val_loss: 0.4145 - val_acc: 0.8717\n",
      "Epoch 94/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2264 - acc: 0.9222 - val_loss: 0.3902 - val_acc: 0.8747\n",
      "Epoch 95/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2218 - acc: 0.9232 - val_loss: 0.4166 - val_acc: 0.8725\n",
      "Epoch 96/100\n",
      "1562/1562 [==============================] - 242s - loss: 0.2215 - acc: 0.9233 - val_loss: 0.4142 - val_acc: 0.8709\n",
      "Epoch 97/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2180 - acc: 0.9236 - val_loss: 0.3971 - val_acc: 0.8756\n",
      "Epoch 98/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2176 - acc: 0.9255 - val_loss: 0.3860 - val_acc: 0.8795\n",
      "Epoch 99/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2178 - acc: 0.9252 - val_loss: 0.4120 - val_acc: 0.8704\n",
      "Epoch 100/100\n",
      "1562/1562 [==============================] - 243s - loss: 0.2201 - acc: 0.9238 - val_loss: 0.5325 - val_acc: 0.8432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdabbd55438>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# initiate Adam opt1mizer\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "#opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "filepath = 'v5-weights.{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "model_chk = ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
    "                save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "csv_log = CSVLogger('v5-training.log')\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[model_chk, csv_log])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V5 - A - Keras CNN with Batch Normalization Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as in the cifar10 CNN example from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are using BatchNormalization in TensorFlow, we set this the axis to 3\n",
    "bn_axis = 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:], name='conv1'))\n",
    "model.add(BatchNormalization(axis=3, name='bn_conv1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), name='conv2'))\n",
    "model.add(BatchNormalization(axis=3, name='bn_conv2'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', name='conv3'))\n",
    "model.add(BatchNormalization(axis=3, name='bn_conv3'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), name='conv4'))\n",
    "model.add(BatchNormalization(axis=3, name='bn_conv4'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, name='fc1'))\n",
    "model.add(BatchNormalization(axis=1, name='bn_fc1'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes, name='output'))\n",
    "model.add(BatchNormalization(axis=1, name='bn_outptut'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define out image processing flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# we will do some mild image pre-processing for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some callbacks, compile and get the training started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1562/1562 [==============================] - 274s - loss: 1.3457 - acc: 0.5337 - val_loss: 1.2060 - val_acc: 0.5939\n",
      "Epoch 2/150\n",
      "1562/1562 [==============================] - 272s - loss: 1.0363 - acc: 0.6464 - val_loss: 0.9069 - val_acc: 0.6919\n",
      "Epoch 3/150\n",
      "1562/1562 [==============================] - 260s - loss: 0.9051 - acc: 0.6895 - val_loss: 0.8789 - val_acc: 0.6956\n",
      "Epoch 4/150\n",
      "1562/1562 [==============================] - 270s - loss: 0.8403 - acc: 0.7110 - val_loss: 0.7062 - val_acc: 0.7575\n",
      "Epoch 5/150\n",
      "1562/1562 [==============================] - 267s - loss: 0.7802 - acc: 0.7323 - val_loss: 0.6751 - val_acc: 0.7740\n",
      "Epoch 6/150\n",
      "1562/1562 [==============================] - 267s - loss: 0.7403 - acc: 0.7462 - val_loss: 0.6216 - val_acc: 0.7847\n",
      "Epoch 7/150\n",
      "1562/1562 [==============================] - 267s - loss: 0.6977 - acc: 0.7594 - val_loss: 0.6294 - val_acc: 0.7817\n",
      "Epoch 8/150\n",
      "1562/1562 [==============================] - 267s - loss: 0.6738 - acc: 0.7676 - val_loss: 0.6170 - val_acc: 0.7880\n",
      "Epoch 9/150\n",
      "1562/1562 [==============================] - 267s - loss: 0.6498 - acc: 0.7750 - val_loss: 0.5633 - val_acc: 0.8072\n",
      "Epoch 10/150\n",
      "1562/1562 [==============================] - 268s - loss: 0.6279 - acc: 0.7843 - val_loss: 0.6289 - val_acc: 0.7867\n",
      "Epoch 11/150\n",
      "1562/1562 [==============================] - 267s - loss: 0.6048 - acc: 0.7931 - val_loss: 0.5433 - val_acc: 0.8171\n",
      "Epoch 12/150\n",
      "1562/1562 [==============================] - 267s - loss: 0.5881 - acc: 0.7982 - val_loss: 0.4860 - val_acc: 0.8351\n",
      "Epoch 13/150\n",
      "1562/1562 [==============================] - 268s - loss: 0.5733 - acc: 0.8042 - val_loss: 0.5529 - val_acc: 0.8166\n",
      "Epoch 14/150\n",
      "1562/1562 [==============================] - 268s - loss: 0.5556 - acc: 0.8094 - val_loss: 0.5294 - val_acc: 0.8207\n",
      "Epoch 15/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.5446 - acc: 0.8130 - val_loss: 0.5004 - val_acc: 0.8302\n",
      "Epoch 16/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.5277 - acc: 0.8181 - val_loss: 0.4925 - val_acc: 0.8297\n",
      "Epoch 17/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.5163 - acc: 0.8219 - val_loss: 0.5128 - val_acc: 0.8274\n",
      "Epoch 18/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.5064 - acc: 0.8250 - val_loss: 0.4610 - val_acc: 0.8445\n",
      "Epoch 19/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4953 - acc: 0.8295 - val_loss: 0.4772 - val_acc: 0.8399\n",
      "Epoch 20/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.4887 - acc: 0.8318 - val_loss: 0.4680 - val_acc: 0.8434\n",
      "Epoch 21/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4772 - acc: 0.8354 - val_loss: 0.4330 - val_acc: 0.8544\n",
      "Epoch 22/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4617 - acc: 0.8413 - val_loss: 0.4835 - val_acc: 0.8333\n",
      "Epoch 23/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4567 - acc: 0.8424 - val_loss: 0.4703 - val_acc: 0.8396\n",
      "Epoch 24/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4506 - acc: 0.8450 - val_loss: 0.4654 - val_acc: 0.8409\n",
      "Epoch 25/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4477 - acc: 0.8461 - val_loss: 0.4490 - val_acc: 0.8457\n",
      "Epoch 26/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4368 - acc: 0.8501 - val_loss: 0.4351 - val_acc: 0.8537\n",
      "Epoch 27/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4290 - acc: 0.8514 - val_loss: 0.4427 - val_acc: 0.8498\n",
      "Epoch 28/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4179 - acc: 0.8571 - val_loss: 0.4378 - val_acc: 0.8538\n",
      "Epoch 29/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4154 - acc: 0.8578 - val_loss: 0.4990 - val_acc: 0.8341\n",
      "Epoch 30/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.4110 - acc: 0.8603 - val_loss: 0.4031 - val_acc: 0.8625\n",
      "Epoch 31/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3996 - acc: 0.8633 - val_loss: 0.5109 - val_acc: 0.8288\n",
      "Epoch 32/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3971 - acc: 0.8627 - val_loss: 0.4617 - val_acc: 0.8499\n",
      "Epoch 33/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3896 - acc: 0.8654 - val_loss: 0.4131 - val_acc: 0.8631\n",
      "Epoch 34/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3824 - acc: 0.8682 - val_loss: 0.4447 - val_acc: 0.8536\n",
      "Epoch 35/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3802 - acc: 0.8686 - val_loss: 0.4352 - val_acc: 0.8545\n",
      "Epoch 36/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3732 - acc: 0.8728 - val_loss: 0.4258 - val_acc: 0.8615\n",
      "Epoch 37/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3686 - acc: 0.8728 - val_loss: 0.4006 - val_acc: 0.8657\n",
      "Epoch 38/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3617 - acc: 0.8759 - val_loss: 0.4562 - val_acc: 0.8505\n",
      "Epoch 39/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3575 - acc: 0.8753 - val_loss: 0.4023 - val_acc: 0.8655\n",
      "Epoch 40/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3564 - acc: 0.8767 - val_loss: 0.4246 - val_acc: 0.8593\n",
      "Epoch 41/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3519 - acc: 0.8780 - val_loss: 0.4114 - val_acc: 0.8631\n",
      "Epoch 42/150\n",
      "1562/1562 [==============================] - 242s - loss: 0.3482 - acc: 0.8808 - val_loss: 0.3978 - val_acc: 0.8712\n",
      "Epoch 43/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3407 - acc: 0.8824 - val_loss: 0.4387 - val_acc: 0.8543\n",
      "Epoch 44/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3395 - acc: 0.8833 - val_loss: 0.4337 - val_acc: 0.8568\n",
      "Epoch 45/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3393 - acc: 0.8818 - val_loss: 0.4241 - val_acc: 0.8600\n",
      "Epoch 46/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3298 - acc: 0.8852 - val_loss: 0.4317 - val_acc: 0.8561\n",
      "Epoch 47/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3331 - acc: 0.8841 - val_loss: 0.4294 - val_acc: 0.8562\n",
      "Epoch 48/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3246 - acc: 0.8884 - val_loss: 0.4130 - val_acc: 0.8663\n",
      "Epoch 49/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3189 - acc: 0.8909 - val_loss: 0.3949 - val_acc: 0.8674\n",
      "Epoch 50/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3192 - acc: 0.8907 - val_loss: 0.4280 - val_acc: 0.8584\n",
      "Epoch 51/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3187 - acc: 0.8892 - val_loss: 0.4002 - val_acc: 0.8709\n",
      "Epoch 52/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.3158 - acc: 0.8905 - val_loss: 0.4480 - val_acc: 0.8522\n",
      "Epoch 53/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.3079 - acc: 0.8935 - val_loss: 0.4031 - val_acc: 0.8720\n",
      "Epoch 54/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.3024 - acc: 0.8960 - val_loss: 0.4291 - val_acc: 0.8608\n",
      "Epoch 55/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.3033 - acc: 0.8947 - val_loss: 0.4301 - val_acc: 0.8577\n",
      "Epoch 56/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.2963 - acc: 0.8979 - val_loss: 0.4003 - val_acc: 0.8645\n",
      "Epoch 57/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.2959 - acc: 0.8973 - val_loss: 0.4028 - val_acc: 0.8695\n",
      "Epoch 58/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.2909 - acc: 0.9002 - val_loss: 0.4098 - val_acc: 0.8661\n",
      "Epoch 59/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.2943 - acc: 0.8985 - val_loss: 0.4352 - val_acc: 0.8610\n",
      "Epoch 60/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.2856 - acc: 0.9016 - val_loss: 0.3879 - val_acc: 0.8747\n",
      "Epoch 61/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.2869 - acc: 0.9014 - val_loss: 0.4009 - val_acc: 0.8700\n",
      "Epoch 62/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.2858 - acc: 0.9018 - val_loss: 0.4067 - val_acc: 0.8705\n",
      "Epoch 63/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.2866 - acc: 0.9015 - val_loss: 0.3911 - val_acc: 0.8749\n",
      "Epoch 64/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 239s - loss: 0.2799 - acc: 0.9018 - val_loss: 0.4106 - val_acc: 0.8664\n",
      "Epoch 65/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2841 - acc: 0.9028 - val_loss: 0.4345 - val_acc: 0.8586\n",
      "Epoch 66/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2749 - acc: 0.9050 - val_loss: 0.3923 - val_acc: 0.8727\n",
      "Epoch 67/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2699 - acc: 0.9060 - val_loss: 0.4109 - val_acc: 0.8738\n",
      "Epoch 68/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2735 - acc: 0.9058 - val_loss: 0.4061 - val_acc: 0.8725\n",
      "Epoch 69/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2679 - acc: 0.9087 - val_loss: 0.4352 - val_acc: 0.8653\n",
      "Epoch 70/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2715 - acc: 0.9058 - val_loss: 0.3955 - val_acc: 0.8749\n",
      "Epoch 71/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2618 - acc: 0.9102 - val_loss: 0.4175 - val_acc: 0.8654\n",
      "Epoch 72/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2630 - acc: 0.9090 - val_loss: 0.4137 - val_acc: 0.8678\n",
      "Epoch 73/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2619 - acc: 0.9088 - val_loss: 0.4232 - val_acc: 0.8675\n",
      "Epoch 74/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2608 - acc: 0.9104 - val_loss: 0.4162 - val_acc: 0.8663\n",
      "Epoch 75/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2585 - acc: 0.9106 - val_loss: 0.4040 - val_acc: 0.8712\n",
      "Epoch 76/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2553 - acc: 0.9119 - val_loss: 0.4280 - val_acc: 0.8668\n",
      "Epoch 77/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2486 - acc: 0.9134 - val_loss: 0.4117 - val_acc: 0.8709\n",
      "Epoch 78/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2480 - acc: 0.9140 - val_loss: 0.4085 - val_acc: 0.8741\n",
      "Epoch 79/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2485 - acc: 0.9139 - val_loss: 0.3988 - val_acc: 0.8738\n",
      "Epoch 80/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2450 - acc: 0.9149 - val_loss: 0.4279 - val_acc: 0.8691\n",
      "Epoch 81/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2477 - acc: 0.9137 - val_loss: 0.4055 - val_acc: 0.8719\n",
      "Epoch 82/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2432 - acc: 0.9154 - val_loss: 0.4044 - val_acc: 0.8733\n",
      "Epoch 83/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2427 - acc: 0.9150 - val_loss: 0.4492 - val_acc: 0.8601\n",
      "Epoch 84/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2391 - acc: 0.9168 - val_loss: 0.4293 - val_acc: 0.8695\n",
      "Epoch 85/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2421 - acc: 0.9166 - val_loss: 0.3957 - val_acc: 0.8750\n",
      "Epoch 86/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2386 - acc: 0.9174 - val_loss: 0.4206 - val_acc: 0.8670\n",
      "Epoch 87/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.2388 - acc: 0.9180 - val_loss: 0.3891 - val_acc: 0.8785\n",
      "Epoch 88/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2357 - acc: 0.9182 - val_loss: 0.4002 - val_acc: 0.8770\n",
      "Epoch 89/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2348 - acc: 0.9190 - val_loss: 0.3947 - val_acc: 0.8776\n",
      "Epoch 90/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2328 - acc: 0.9203 - val_loss: 0.4134 - val_acc: 0.8738\n",
      "Epoch 91/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2333 - acc: 0.9195 - val_loss: 0.3927 - val_acc: 0.8798\n",
      "Epoch 92/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2325 - acc: 0.9199 - val_loss: 0.3915 - val_acc: 0.8767\n",
      "Epoch 93/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2282 - acc: 0.9217 - val_loss: 0.4181 - val_acc: 0.8671\n",
      "Epoch 94/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2242 - acc: 0.9206 - val_loss: 0.4289 - val_acc: 0.8678\n",
      "Epoch 95/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2253 - acc: 0.9222 - val_loss: 0.3943 - val_acc: 0.8761\n",
      "Epoch 96/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2262 - acc: 0.9213 - val_loss: 0.3940 - val_acc: 0.8757\n",
      "Epoch 97/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2202 - acc: 0.9243 - val_loss: 0.4426 - val_acc: 0.8602\n",
      "Epoch 98/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2231 - acc: 0.9232 - val_loss: 0.3978 - val_acc: 0.8756\n",
      "Epoch 99/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2183 - acc: 0.9241 - val_loss: 0.4212 - val_acc: 0.8684\n",
      "Epoch 100/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2237 - acc: 0.9218 - val_loss: 0.3988 - val_acc: 0.8749\n",
      "Epoch 101/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2164 - acc: 0.9251 - val_loss: 0.4043 - val_acc: 0.8704\n",
      "Epoch 102/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2192 - acc: 0.9242 - val_loss: 0.3973 - val_acc: 0.8795\n",
      "Epoch 103/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2175 - acc: 0.9237 - val_loss: 0.3949 - val_acc: 0.8776\n",
      "Epoch 104/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2156 - acc: 0.9252 - val_loss: 0.4146 - val_acc: 0.8712\n",
      "Epoch 105/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2167 - acc: 0.9253 - val_loss: 0.4239 - val_acc: 0.8692\n",
      "Epoch 106/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2120 - acc: 0.9272 - val_loss: 0.3949 - val_acc: 0.8732\n",
      "Epoch 107/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2123 - acc: 0.9264 - val_loss: 0.4041 - val_acc: 0.8748\n",
      "Epoch 108/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2063 - acc: 0.9292 - val_loss: 0.4712 - val_acc: 0.8609\n",
      "Epoch 109/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2093 - acc: 0.9266 - val_loss: 0.3943 - val_acc: 0.8758\n",
      "Epoch 110/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2106 - acc: 0.9278 - val_loss: 0.4071 - val_acc: 0.8779\n",
      "Epoch 111/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2104 - acc: 0.9268 - val_loss: 0.4032 - val_acc: 0.8777\n",
      "Epoch 112/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2082 - acc: 0.9284 - val_loss: 0.4217 - val_acc: 0.8706\n",
      "Epoch 113/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.2067 - acc: 0.9284 - val_loss: 0.4148 - val_acc: 0.8721\n",
      "Epoch 114/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2020 - acc: 0.9291 - val_loss: 0.4249 - val_acc: 0.8719\n",
      "Epoch 115/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.2019 - acc: 0.9302 - val_loss: 0.4264 - val_acc: 0.8690\n",
      "Epoch 116/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.2025 - acc: 0.9295 - val_loss: 0.4160 - val_acc: 0.8731\n",
      "Epoch 117/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.2002 - acc: 0.9317 - val_loss: 0.4260 - val_acc: 0.8729\n",
      "Epoch 118/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.1997 - acc: 0.9312 - val_loss: 0.4138 - val_acc: 0.8723\n",
      "Epoch 119/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.1986 - acc: 0.9311 - val_loss: 0.4223 - val_acc: 0.8717\n",
      "Epoch 120/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.1964 - acc: 0.9321 - val_loss: 0.4002 - val_acc: 0.8770\n",
      "Epoch 121/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.2020 - acc: 0.9293 - val_loss: 0.3981 - val_acc: 0.8781\n",
      "Epoch 122/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.1968 - acc: 0.9326 - val_loss: 0.4115 - val_acc: 0.8747\n",
      "Epoch 123/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.1950 - acc: 0.9329 - val_loss: 0.4331 - val_acc: 0.8719\n",
      "Epoch 124/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.1966 - acc: 0.9310 - val_loss: 0.4245 - val_acc: 0.8737\n",
      "Epoch 125/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.1906 - acc: 0.9339 - val_loss: 0.4162 - val_acc: 0.8749\n",
      "Epoch 126/150\n",
      "1562/1562 [==============================] - 241s - loss: 0.1908 - acc: 0.9335 - val_loss: 0.4706 - val_acc: 0.8604\n",
      "Epoch 127/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 239s - loss: 0.1889 - acc: 0.9346 - val_loss: 0.4664 - val_acc: 0.8596\n",
      "Epoch 128/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1920 - acc: 0.9332 - val_loss: 0.4206 - val_acc: 0.8719\n",
      "Epoch 129/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1936 - acc: 0.9326 - val_loss: 0.4310 - val_acc: 0.8655\n",
      "Epoch 130/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1919 - acc: 0.9332 - val_loss: 0.4173 - val_acc: 0.8760\n",
      "Epoch 131/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1922 - acc: 0.9327 - val_loss: 0.4122 - val_acc: 0.8744\n",
      "Epoch 132/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1888 - acc: 0.9350 - val_loss: 0.4197 - val_acc: 0.8733\n",
      "Epoch 133/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1877 - acc: 0.9349 - val_loss: 0.4236 - val_acc: 0.8693\n",
      "Epoch 134/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.1867 - acc: 0.9354 - val_loss: 0.4019 - val_acc: 0.8772\n",
      "Epoch 135/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1858 - acc: 0.9349 - val_loss: 0.4371 - val_acc: 0.8745\n",
      "Epoch 136/150\n",
      "1562/1562 [==============================] - 240s - loss: 0.1874 - acc: 0.9350 - val_loss: 0.4165 - val_acc: 0.8761\n",
      "Epoch 137/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1849 - acc: 0.9353 - val_loss: 0.4019 - val_acc: 0.8785\n",
      "Epoch 138/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1804 - acc: 0.9374 - val_loss: 0.4102 - val_acc: 0.8783\n",
      "Epoch 139/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1831 - acc: 0.9371 - val_loss: 0.4150 - val_acc: 0.8743\n",
      "Epoch 140/150\n",
      "1562/1562 [==============================] - 239s - loss: 0.1828 - acc: 0.9378 - val_loss: 0.4184 - val_acc: 0.8752\n",
      "Epoch 141/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.1864 - acc: 0.9348 - val_loss: 0.4063 - val_acc: 0.8808\n",
      "Epoch 142/150\n",
      "1562/1562 [==============================] - 246s - loss: 0.1810 - acc: 0.9378 - val_loss: 0.4133 - val_acc: 0.8793\n",
      "Epoch 143/150\n",
      "1562/1562 [==============================] - 245s - loss: 0.1808 - acc: 0.9375 - val_loss: 0.4465 - val_acc: 0.8635\n",
      "Epoch 144/150\n",
      "1562/1562 [==============================] - 242s - loss: 0.1809 - acc: 0.9379 - val_loss: 0.4158 - val_acc: 0.8807\n",
      "Epoch 145/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.1772 - acc: 0.9383 - val_loss: 0.4300 - val_acc: 0.8760\n",
      "Epoch 146/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.1768 - acc: 0.9392 - val_loss: 0.4096 - val_acc: 0.8776\n",
      "Epoch 147/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.1860 - acc: 0.9360 - val_loss: 0.4367 - val_acc: 0.8663\n",
      "Epoch 148/150\n",
      "1562/1562 [==============================] - 242s - loss: 0.1742 - acc: 0.9400 - val_loss: 0.4694 - val_acc: 0.8644\n",
      "Epoch 149/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.1783 - acc: 0.9387 - val_loss: 0.4202 - val_acc: 0.8770\n",
      "Epoch 150/150\n",
      "1562/1562 [==============================] - 243s - loss: 0.1775 - acc: 0.9379 - val_loss: 0.4377 - val_acc: 0.8707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8fbad7b00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 150\n",
    "\n",
    "# initiate Adam opt1mizer\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "#opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "filepath = 'v5a-weights.{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "model_chk = ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
    "                save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "csv_log = CSVLogger('v5a-training.log')\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[model_chk, csv_log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
